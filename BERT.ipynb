{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from First_Dataset import First_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = First_Dataset()\n",
    "dataset.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "total_sentences = list(dataset.df['tweet'].values)\n",
    "total_labels = list(dataset.df['class'].values)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(total_labels)\n",
    "encoded_labels = le.transform(total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 486kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.22MB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length is:  159\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 0\n",
    "for sentence in total_sentences:\n",
    "    #print(sentence)\n",
    "    length = len(tokenizer.tokenize(sentence))\n",
    "    if length > max_length:\n",
    "        max_length  = length\n",
    "print(\"max token length is: \",max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_Preprocessing import *\n",
    "\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = split_data(total_sentences,total_labels,0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def encoder_generator(sentences,labels):\n",
    "    \n",
    "    sent_index = []\n",
    "    input_ids = []\n",
    "    attention_masks =[]\n",
    "\n",
    "    for index,sent in enumerate(sentences):\n",
    "        \n",
    "        sent_index.append(index)\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(sent,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=160,\n",
    "                                             pad_to_max_length=True,\n",
    "                                             truncation = True,\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_tensors='pt')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids,dim=0)\n",
    "    attention_masks = torch.cat(attention_masks,dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    sent_index = torch.tensor(sent_index)\n",
    "\n",
    "    return sent_index,input_ids,attention_masks,labels\n",
    "\n",
    "sent_index,input_ids,attention_masks,encoded_label_tensors = encoder_generator(X_train,Y_train)\n",
    "val_sent_index,val_input_ids,val_attention_masks,encoded_val_label_tensors = encoder_generator(X_val,Y_val)\n",
    "test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors = encoder_generator(X_test,Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,random_split\n",
    "\n",
    "train_dataset = TensorDataset(input_ids,attention_masks,encoded_label_tensors)\n",
    "val_dataset = TensorDataset(val_sent_index,val_input_ids,val_attention_masks,encoded_val_label_tensors)\n",
    "test_dataset = TensorDataset(test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "bs=8\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                              sampler=RandomSampler(train_dataset),\n",
    "                              batch_size=bs)\n",
    "valid_data_loader = DataLoader(val_dataset,\n",
    "                              sampler=SequentialSampler(val_dataset),\n",
    "                              batch_size=bs)\n",
    "test_data_loader = DataLoader(test_dataset,\n",
    "                            sampler=SequentialSampler(test_dataset),\n",
    "                            batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 440M/440M [04:12<00:00, 1.75MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                     num_labels=len(le.classes_),\n",
    "                                                     output_attentions=False,\n",
    "                                                     output_hidden_states=False,\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "output = model(input_ids, attention_mask=attention_masks)[0]\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
    "    {'output_1': train_labels},\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_data=(\n",
    "        {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},\n",
    "        {'output_1': test_labels}\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
